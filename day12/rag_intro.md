# ğŸ“˜ Study Notes: System Prompts, One-Shot Prompting & the First Step Toward RAG

---

## ğŸ”¹ 1. Improving the System Prompt

### ğŸ”¸ What Is a System Prompt?

The **system prompt**:

- Sets the role of the model
- Defines tone and behavior
- Injects business rules
- Shapes how the assistant responds

ğŸ“Œ It controls the assistant before the user even speaks.

---

### ğŸ”¸ Example System Prompt (Clothing Store)

The assistant is told:

- You are a **helpful assistant in a clothes store**
- Encourage customers to buy **items on sale**
- Hats are **60% off**
- Most other items are **50% or 60% off**
- Encourage hats when unsure

This prompt includes:

| Component        | Purpose               |
| ---------------- | --------------------- |
| Context          | Clothes store         |
| Business Rules   | Hats 60% off          |
| Tone             | Gentle encouragement  |
| Strategy         | Upsell hats           |
| Example Response | Demonstrates behavior |

---

## ğŸ¯ 2. One-Shot Prompting

### ğŸ”¸ What Is One-Shot Prompting?

**One-shot prompting**:

- Provide **one example**
- Model learns the pattern
- Replicates similar structure

Example:

> If customer says: â€œI want a hatâ€
> Reply with: â€œWonderful! We have hats on saleâ€¦â€

ğŸ“Œ The example teaches the model the desired format.

---

## ğŸ”„ 3. Multi-Shot Prompting

### ğŸ”¸ What Is Multi-Shot Prompting?

When you provide:

- Multiple example scenarios
- Multiple example responses

Example addition:

> If customer asks for shoes â†’
> Say shoes are not on sale â†’ Suggest hats

Now the model has:

- Multiple behavioral examples
- More structured guidance

ğŸ“Œ More examples = clearer behavior control.

---

## ğŸ§  4. Dynamic System Prompts (Context Injection)

### ğŸ”¸ What Was Demonstrated?

System prompt changes dynamically based on user input:

```python
if "belt" in user_message:
    system_prompt += "The store does not sell belts..."
```

---

### ğŸ”¸ What Happens?

If user asks:

> â€œI want a beltâ€

Then the system prompt:

- Gets extra information
- Tells the model:
  - Store doesnâ€™t sell belts
  - Suggest other items

ğŸ“Œ The model behaves differently because the **prompt changed**.

---

## ğŸ“¦ 8. Why Not Just Add Everything to the Prompt?

You _could_ always include:

- All products
- All rules
- All pricing
- All store policies

But problems:

- Context window grows too large
- Token costs increase
- Accuracy degrades
- Noise increases

ğŸ“Œ Selective context injection is smarter.

---

## ğŸ§  9. This Is Inference-Time Scaling

Important distinction:

| Training-Time Scaling | Inference-Time Scaling |
| --------------------- | ---------------------- |
| Bigger model          | Better prompting       |
| More parameters       | More relevant context  |
| Expensive             | Efficient              |

This lab demonstrates:

- **Inference-time scaling**
- Same model
- Smarter prompt usage

---

## ğŸ¢ 10. Business Applications

This technique enables:

### ğŸ”¹ Conversational Agents

- Customer support bots
- Sales assistants
- Product recommendation bots

---

### ğŸ”¹ Adding Business Value

Instead of:

> â€œJust use ChatGPTâ€

You:

- Inject proprietary business knowledge
- Control tone
- Control behavior
- Insert company rules

ğŸ“Œ Thatâ€™s where companies add value.

---

## ğŸš€ 11. Key Engineering Insight

LLMs are:

- Powerful out of the box

But become:

- Business-grade tools
- When you add context + rules

Prompt = Knowledge + Control + Strategy

---

# âš¡ Quick Revision

- **System prompt** defines behavior
- **One-shot prompting** = one example
- **Multi-shot prompting** = multiple examples
- Dynamic prompt updates = context injection
- This is the foundation of **RAG**
- RAG = retrieve â†’ insert â†’ generate
- Better context = better outputs
- Business value comes from domain-specific context

---

# 1ï¸âƒ£ RAG (Retrieval-Augmented Generation)

---

## ğŸ”¹ Simple Definition

RAG = LLM + Your External Data

It allows a language model to **search documents first**, then generate an answer using that retrieved information.

---

## ğŸ”¹ Why RAG is Needed

Problem with normal LLM:

- It only knows what it was trained on.
- It can hallucinate.
- It doesn't know your private data (PDFs, company DB, etc.).

RAG solves this by:

- Fetching relevant documents
- Feeding them into the model
- Generating accurate answers

---

## ğŸ”¹ Architecture Diagram (Explained in Words)

Imagine this pipeline:

User Question
â†“
Convert question into vector
â†“
Search vector database
â†“
Retrieve relevant documents
â†“
Send documents + question to LLM
â†“
Generate final answer

That is RAG.

---

## ğŸ”¹ Components Involved

1. **Documents** â€“ PDFs, DB data, websites
2. **Embedding Model** â€“ Converts text into vectors
3. **Vector Database** â€“ Stores embeddings (e.g., FAISS, Pinecone)
4. **Retriever** â€“ Finds similar documents
5. **LLM** â€“ Generates final answer
6. **Prompt Template** â€“ Controls how answer is generated

---

## ğŸ”¹ Data Flow Step-by-Step

1. You upload documents.
2. Documents are split into chunks.
3. Each chunk is converted into embeddings.
4. Stored inside a vector database.
5. User asks a question.
6. Question is converted into embedding.
7. Vector DB finds similar chunks.
8. Those chunks are sent to LLM.
9. LLM generates answer based only on retrieved data.

---

## ğŸ”¹ Python Pseudo Code

```python
# Step 1: Load documents
docs = load_documents("company_data.pdf")

# Step 2: Split documents
chunks = split_into_chunks(docs)

# Step 3: Create embeddings
embeddings = embed_model.embed(chunks)

# Step 4: Store in vector DB
vector_db.store(chunks, embeddings)

# Step 5: User question
question = "What is refund policy?"

# Step 6: Retrieve relevant chunks
relevant_docs = vector_db.search(question)

# Step 7: Send to LLM
response = llm.generate(
    prompt=f"Answer using this context: {relevant_docs}\nQuestion: {question}"
)

print(response)
```

---

## ğŸ”¹ When to Use RAG

âœ… Chatbot using company data
âœ… Legal document Q&A
âœ… Medical research assistant
âœ… Internal enterprise knowledge base
âœ… AI tutor with textbooks

---

## ğŸ”¹ When NOT to Use RAG

âŒ Simple chatbot with no external data
âŒ Small static FAQs
âŒ When latency must be extremely low

---

## ğŸ”¹ Common Interview Questions

1. What problem does RAG solve?
2. Difference between fine-tuning and RAG?
3. What is embedding?
4. What is vector similarity search?
5. How do you reduce hallucination in RAG?
6. What is chunking strategy?
7. What is context window limitation?

---

# 2ï¸âƒ£ LangChain

---

## ğŸ”¹ Simple Definition

LangChain is a framework to build applications using LLMs.

It helps you connect:

- LLMs
- Tools
- Memory
- Vector databases
- Agents

---

## ğŸ”¹ Why LangChain is Needed

LLM alone = just text generation.

Real apps need:

- Retrieval
- Memory
- Tool calling
- Multi-step reasoning

LangChain organizes this.

---

## ğŸ”¹ Architecture Diagram (In Words)

User
â†“
Prompt Template
â†“
Chain
â†“
LLM
â†“
Output

OR (Advanced)

User
â†“
Agent
â†“
Tool Selection
â†“
Call Tool
â†“
Return Result
â†“
LLM Final Response

---

## ğŸ”¹ Components

1. **LLM Wrapper**
2. **PromptTemplate**
3. **Chains**
4. **Memory**
5. **Agents**
6. **Tools**
7. **Vector Stores**

---

## ğŸ”¹ Data Flow Step-by-Step (Simple Chain)

1. User sends input.
2. Prompt template formats it.
3. Chain sends it to LLM.
4. LLM generates response.
5. Output returned.

---

## ğŸ”¹ Python Pseudo Code

```python
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

llm = OpenAI()

template = "Explain {topic} in simple terms."
prompt = PromptTemplate(template=template)

chain = LLMChain(llm=llm, prompt=prompt)

response = chain.run(topic="RAG")

print(response)
```

---

## ğŸ”¹ When to Use LangChain

âœ… Building RAG systems
âœ… Tool-calling chatbots
âœ… Multi-step workflows
âœ… AI agents
âœ… Chatbots with memory

---

## ğŸ”¹ When NOT to Use LangChain

âŒ Simple API call to GPT
âŒ Very small experimental scripts
âŒ When you need ultra-lightweight system

---

## ğŸ”¹ Common Interview Questions

1. What is a Chain?
2. Difference between Agent and Chain?
3. What is Memory?
4. What is Tool calling?
5. What is Runnable?
6. How does LangChain support RAG?

---

# 3ï¸âƒ£ Agentic AI

---

## ğŸ”¹ Simple Definition

Agentic AI = AI that can think, decide, and take actions autonomously.

Instead of just answering,
it can:

- Plan
- Use tools
- Make decisions
- Execute steps

---

## ğŸ”¹ Why Agentic AI is Needed

Normal LLM:
User â†’ Answer

Agentic AI:
User â†’ Plan â†’ Tool â†’ Analyze â†’ Decide â†’ Repeat â†’ Final answer

Used for automation and complex workflows.

---

## ğŸ”¹ Architecture Diagram (In Words)

User Goal
â†“
Planner (LLM)
â†“
Select Tool
â†“
Execute Tool
â†“
Observe Result
â†“
Decide Next Step
â†“
Repeat until goal achieved
â†“
Final Output

This is called a **Reason-Act-Observe loop**.

---

## ğŸ”¹ Components

1. LLM (Reasoning brain)
2. Tools (APIs, DB, Calculator)
3. Memory
4. Planner
5. Execution Engine

---

## ğŸ”¹ Data Flow Step-by-Step

Example: â€œBook cheapest flight tomorrowâ€

1. Agent understands goal.
2. Decides to call flight search API.
3. Gets flight data.
4. Compares prices.
5. Chooses cheapest.
6. Calls booking API.
7. Confirms booking.

Multiple thinking steps.

---

## ğŸ”¹ Python Pseudo Code

```python
while not task_completed:

    plan = llm.generate("What should I do next?")

    if "search flight" in plan:
        result = search_flight_api()

    elif "compare prices" in plan:
        result = compare_prices()

    elif "book flight" in plan:
        result = book_ticket()
        task_completed = True

print("Task Completed")
```

---

## ğŸ”¹ When to Use Agentic AI

âœ… Workflow automation
âœ… Autonomous research assistant
âœ… Trading bots
âœ… AI copilots
âœ… Complex multi-step decision systems

---

## ğŸ”¹ When NOT to Use

âŒ Simple FAQ chatbot
âŒ Single-step question answering
âŒ Low latency critical systems

---

## ğŸ”¹ Common Interview Questions

1. What is an AI Agent?
2. What is ReAct pattern?
3. Difference between RAG and Agent?
4. What is tool calling?
5. What is memory in agents?
6. Risks of autonomous AI systems?

---

# ğŸ”¥ Final Comparison

| Feature              | RAG                    | LangChain      | Agentic AI                 |
| -------------------- | ---------------------- | -------------- | -------------------------- |
| Purpose              | Add external knowledge | Build LLM apps | Autonomous decision-making |
| Uses Vector DB       | Yes                    | Optional       | Optional                   |
| Uses Tools           | No (basic RAG)         | Yes            | Yes                        |
| Multi-step reasoning | No                     | Limited        | Yes                        |
| Autonomous           | No                     | Sometimes      | Yes                        |

---

# ğŸ§  Real-World Perspective

- RAG = Knowledge
- LangChain = Framework
- Agentic AI = Autonomy

---
