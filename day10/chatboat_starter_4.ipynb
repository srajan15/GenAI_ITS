{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import gradio as gr\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gemma3:270m\"\n",
    "# Again, I'll be in scientist-mode and change this global during the lab\n",
    "\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "## And now, writing a new callback\n",
    "\n",
    "We now need to write a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "Which will be a callback function we will give gradio.\n",
    "\n",
    "### The job of this function\n",
    "\n",
    "Take a message, take the prior conversation, and return the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "354ce793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    return \"bananas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e87f3417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4996e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def chat(message, history):\n",
    "    with open(\"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "    return f\"You said {message} and the history is {history} but I still say bananas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "434a0417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890cac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7330d7f",
   "metadata": {},
   "source": [
    "## OK! Let's write a slightly better chat callback!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    print(message)\n",
    "    \n",
    "    #  Extract text safely (new Gradio format)\n",
    "    if isinstance(message, dict):\n",
    "        message_text = message[\"content\"][0][\"text\"]\n",
    "    elif isinstance(message, list):\n",
    "        message_text = message[0][\"text\"]\n",
    "    else:\n",
    "        message_text = message\n",
    "\n",
    "    # Convert history to Ollama format\n",
    "    formatted_history = []\n",
    "\n",
    "    for msg in history:\n",
    "        if isinstance(msg[\"content\"], list):\n",
    "            text = msg[\"content\"][0][\"text\"]\n",
    "        else:\n",
    "            text = msg[\"content\"]\n",
    "\n",
    "        formatted_history.append({\n",
    "            \"role\": msg[\"role\"],\n",
    "            \"content\": text\n",
    "        })\n",
    "\n",
    "    messages = (\n",
    "        [{\"role\": \"system\", \"content\": system_message}]\n",
    "        + formatted_history\n",
    "        + [{\"role\": \"user\", \"content\": message_text}]\n",
    "    )\n",
    "\n",
    "    response = ollama.chat(model=MODEL, messages=messages)\n",
    "    \n",
    "    #  Return in Gradio expected format\n",
    "    return {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response[\"message\"][\"content\"]\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a0cce",
   "metadata": {},
   "source": [
    "#  The Code\n",
    "\n",
    "```python\n",
    "if isinstance(message, dict):\n",
    "    message_text = message[\"content\"][0][\"text\"]\n",
    "elif isinstance(message, list):\n",
    "    message_text = message[0][\"text\"]\n",
    "else:\n",
    "    message_text = message\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#  First Understand the Problem\n",
    "\n",
    "In newer versions of **Gradio**, the message can be:\n",
    "\n",
    "### Case 1 â€” Dict format\n",
    "\n",
    "```python\n",
    "{\n",
    "   \"role\": \"user\",\n",
    "   \"content\": [\n",
    "       {\"type\": \"text\", \"text\": \"hello\"}\n",
    "   ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Case 2 â€” List format\n",
    "\n",
    "```python\n",
    "[\n",
    "   {\"type\": \"text\", \"text\": \"hello\"}\n",
    "]\n",
    "```\n",
    "\n",
    "### Case 3 â€” Plain string\n",
    "\n",
    "```python\n",
    "\"hello\"\n",
    "```\n",
    "\n",
    "Your model (Ollama, OpenAI, etc.) expects:\n",
    "\n",
    "```python\n",
    "\"hello\"\n",
    "```\n",
    "\n",
    "So we must extract the text safely.\n",
    "\n",
    "---\n",
    "\n",
    "# Line-by-Line Deep Explanation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1ï¸âƒ£ `isinstance(message, dict)`\n",
    "\n",
    "### What is `isinstance()`?\n",
    "\n",
    "Python built-in function:\n",
    "\n",
    "```python\n",
    "isinstance(object, datatype)\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "* `True` â†’ if object matches datatype\n",
    "* `False` â†’ otherwise\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "isinstance(\"hello\", str)  # True\n",
    "isinstance(5, int)        # True\n",
    "isinstance([], list)      # True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Here:\n",
    "\n",
    "```python\n",
    "if isinstance(message, dict):\n",
    "```\n",
    "\n",
    "We check:\n",
    "\n",
    "ðŸ‘‰ Is message a dictionary?\n",
    "\n",
    "---\n",
    "\n",
    "### Example Input\n",
    "\n",
    "```python\n",
    "message = {\n",
    "   \"role\": \"user\",\n",
    "   \"content\": [\n",
    "       {\"type\": \"text\", \"text\": \"Explain AI\"}\n",
    "   ]\n",
    "}\n",
    "```\n",
    "\n",
    "This is a `dict`.\n",
    "\n",
    "So this block runs:\n",
    "\n",
    "```python\n",
    "message_text = message[\"content\"][0][\"text\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Breakdown\n",
    "\n",
    "```python\n",
    "message[\"content\"]\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "```python\n",
    "[\n",
    "   {\"type\": \"text\", \"text\": \"Explain AI\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "message[\"content\"][0]\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "```python\n",
    "{\"type\": \"text\", \"text\": \"Explain AI\"}\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "[\"text\"]\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "```python\n",
    "\"Explain AI\"\n",
    "```\n",
    "\n",
    "So final:\n",
    "\n",
    "```python\n",
    "message_text = \"Explain AI\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ 2ï¸âƒ£ `elif isinstance(message, list)`\n",
    "\n",
    "Now suppose message is:\n",
    "\n",
    "```python\n",
    "message = [\n",
    "   {\"type\": \"text\", \"text\": \"Hello\"}\n",
    "]\n",
    "```\n",
    "\n",
    "This is a list.\n",
    "\n",
    "So second condition runs:\n",
    "\n",
    "```python\n",
    "message_text = message[0][\"text\"]\n",
    "```\n",
    "\n",
    "Breakdown:\n",
    "\n",
    "```python\n",
    "message[0]\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "```python\n",
    "{\"type\": \"text\", \"text\": \"Hello\"}\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "[\"text\"]\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "```python\n",
    "\"Hello\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ 3ï¸âƒ£ Else Condition\n",
    "\n",
    "```python\n",
    "else:\n",
    "    message_text = message\n",
    "```\n",
    "\n",
    "This handles:\n",
    "\n",
    "```python\n",
    "message = \"Hello\"\n",
    "```\n",
    "\n",
    "Since it's already string:\n",
    "\n",
    "We don't need to extract anything.\n",
    "\n",
    "---\n",
    "\n",
    "#  Why This Code Is Important\n",
    "\n",
    "Without this safety check, you might do:\n",
    "\n",
    "```python\n",
    "message_text = message[\"content\"][0][\"text\"]\n",
    "```\n",
    "\n",
    "But if message is:\n",
    "\n",
    "```python\n",
    "\"hello\"\n",
    "```\n",
    "\n",
    "Python throws:\n",
    "\n",
    "```\n",
    "TypeError: string indices must be integers\n",
    "```\n",
    "\n",
    "Or if message is list:\n",
    "\n",
    "```\n",
    "TypeError: list indices must be integers\n",
    "```\n",
    "\n",
    "This code prevents runtime crashes.\n",
    "\n",
    "---\n",
    "\n",
    "#  Real Execution Flow Example\n",
    "\n",
    "Suppose Gradio sends:\n",
    "\n",
    "```python\n",
    "message = [\n",
    "   {\"type\": \"text\", \"text\": \"What is Python?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Execution:\n",
    "\n",
    "```\n",
    "isinstance(dict)? â†’ False\n",
    "isinstance(list)? â†’ True\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "message_text = \"What is Python?\"\n",
    "```\n",
    "\n",
    "Now safe to send to model.\n",
    "\n",
    "---\n",
    "\n",
    "#  Why Multiple Formats Exist?\n",
    "\n",
    "Because new Gradio supports:\n",
    "\n",
    "* Text\n",
    "* Images\n",
    "* Audio\n",
    "* Files\n",
    "* Multimodal messages\n",
    "\n",
    "So content is stored as:\n",
    "\n",
    "```python\n",
    "[\n",
    "   {\"type\": \"text\", \"text\": \"...\"},\n",
    "   {\"type\": \"image\", \"url\": \"...\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Your code extracts only text part.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#  Concept Behind This Pattern\n",
    "\n",
    "This is called:\n",
    "\n",
    "> Defensive Programming\n",
    "\n",
    "We handle all possible input types safely.\n",
    "\n",
    "Very important in production systems.\n",
    "\n",
    "---\n",
    "\n",
    "#  Interview-Level Explanation (Short Version)\n",
    "\n",
    "If interviewer asks:\n",
    "\n",
    "Why use `isinstance()` here?\n",
    "\n",
    "Answer:\n",
    "\n",
    "1. Gradio may send message in dict, list, or string format.\n",
    "2. LLM APIs expect plain string content.\n",
    "3. This ensures safe extraction and prevents type errors.\n",
    "\n",
    "---\n",
    "\n",
    "#  Final Mental Model\n",
    "\n",
    "Think like this:\n",
    "\n",
    "```\n",
    "Gradio gives â†’ Complex object\n",
    "LLM needs   â†’ Simple string\n",
    "This block  â†’ Converts safely\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d68f9",
   "metadata": {},
   "source": [
    "This block is responsible for:\n",
    "\n",
    "> Converting Gradioâ€™s chat history into clean LLM-compatible format.\n",
    "\n",
    "Letâ€™s break it down **step-by-step with real examples**.\n",
    "\n",
    "---\n",
    "\n",
    "#  The Code\n",
    "\n",
    "```python\n",
    "formatted_history = []\n",
    "\n",
    "for msg in history:\n",
    "    if isinstance(msg[\"content\"], list):\n",
    "        text = msg[\"content\"][0][\"text\"]\n",
    "    else:\n",
    "        text = msg[\"content\"]\n",
    "\n",
    "    formatted_history.append({\n",
    "        \"role\": msg[\"role\"],\n",
    "        \"content\": text\n",
    "    })\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#  First Understand: What Is `history`?\n",
    "\n",
    "In **Gradio ChatInterface**,\n",
    "`history` is a list of previous messages.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Hi\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Hello!\"}]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "Notice:\n",
    "\n",
    "ðŸ‘‰ `content` is a **list** (because Gradio supports multimodal input)\n",
    "\n",
    "But your LLM (Ollama/OpenAI/etc.) expects:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello!\"}\n",
    "]\n",
    "```\n",
    "\n",
    "So we must clean it.\n",
    "\n",
    "---\n",
    "\n",
    "#  Line-by-Line Explanation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1ï¸âƒ£ Create Empty List\n",
    "\n",
    "```python\n",
    "formatted_history = []\n",
    "```\n",
    "\n",
    "We create a new list to store cleaned messages.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because we donâ€™t want to modify original history directly.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2ï¸âƒ£ Loop Through Each Message\n",
    "\n",
    "```python\n",
    "for msg in history:\n",
    "```\n",
    "\n",
    "Each `msg` looks like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": \"Hi\"}]\n",
    "}\n",
    "```\n",
    "\n",
    "First loop:\n",
    "\n",
    "```\n",
    "msg = {\"role\": \"user\", \"content\": [...]}\n",
    "```\n",
    "\n",
    "Second loop:\n",
    "\n",
    "```\n",
    "msg = {\"role\": \"assistant\", \"content\": [...]}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3ï¸âƒ£ Check Content Type\n",
    "\n",
    "```python\n",
    "if isinstance(msg[\"content\"], list):\n",
    "```\n",
    "\n",
    "Why check this?\n",
    "\n",
    "Because Gradio content may be:\n",
    "\n",
    "### Case A â€” List (multimodal format)\n",
    "\n",
    "```python\n",
    "[\n",
    "   {\"type\": \"text\", \"text\": \"Hi\"}\n",
    "]\n",
    "```\n",
    "\n",
    "### Case B â€” Already string\n",
    "\n",
    "```python\n",
    "\"Hi\"\n",
    "```\n",
    "\n",
    "We handle both safely.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4ï¸âƒ£ Extract Text If Itâ€™s List\n",
    "\n",
    "```python\n",
    "text = msg[\"content\"][0][\"text\"]\n",
    "```\n",
    "\n",
    "Step-by-step:\n",
    "\n",
    "```python\n",
    "msg[\"content\"]\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "```python\n",
    "[\n",
    "   {\"type\": \"text\", \"text\": \"Hi\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "[0]\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "```python\n",
    "{\"type\": \"text\", \"text\": \"Hi\"}\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "[\"text\"]\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "```python\n",
    "\"Hi\"\n",
    "```\n",
    "\n",
    "So now:\n",
    "\n",
    "```python\n",
    "text = \"Hi\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5ï¸âƒ£ Else Case\n",
    "\n",
    "```python\n",
    "else:\n",
    "    text = msg[\"content\"]\n",
    "```\n",
    "\n",
    "If content is already string:\n",
    "\n",
    "```python\n",
    "msg = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Hello!\"\n",
    "}\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "text = \"Hello!\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 6ï¸âƒ£ Append Clean Message\n",
    "\n",
    "```python\n",
    "formatted_history.append({\n",
    "    \"role\": msg[\"role\"],\n",
    "    \"content\": text\n",
    "})\n",
    "```\n",
    "\n",
    "This creates:\n",
    "\n",
    "```python\n",
    "{\n",
    "   \"role\": \"user\",\n",
    "   \"content\": \"Hi\"\n",
    "}\n",
    "```\n",
    "\n",
    "And adds it to new list.\n",
    "\n",
    "---\n",
    "\n",
    "#  Final Output Example\n",
    "\n",
    "### Original `history`\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Hi\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Hello!\"}]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "### After Processing\n",
    "\n",
    "```python\n",
    "formatted_history = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello!\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Now this is compatible with:\n",
    "\n",
    "* Ollama\n",
    "* OpenAI\n",
    "* Mistral\n",
    "* DeepSeek\n",
    "* Most modern LLM APIs\n",
    "\n",
    "---\n",
    "\n",
    "#  Why This Is Very Important\n",
    "\n",
    "If you donâ€™t clean it:\n",
    "\n",
    "Your API call receives:\n",
    "\n",
    "```python\n",
    "\"content\": [{\"type\": \"text\", \"text\": \"Hi\"}]\n",
    "```\n",
    "\n",
    "But LLM expects:\n",
    "\n",
    "```python\n",
    "\"content\": \"Hi\"\n",
    "```\n",
    "\n",
    "Then you get:\n",
    "\n",
    "```\n",
    "ValidationError: content must be string\n",
    "```\n",
    "\n",
    "Which you were seeing earlier.\n",
    "\n",
    "---\n",
    "\n",
    "#  Concept: Data Normalization\n",
    "\n",
    "This block performs:\n",
    "\n",
    "> Input Normalization\n",
    "\n",
    "We convert UI-specific structure â†’ Model-compatible structure.\n",
    "\n",
    "This is very common in production systems.\n",
    "\n",
    "---\n",
    "\n",
    "#  Architecture View\n",
    "\n",
    "```\n",
    "Gradio History (complex)\n",
    "        â†“\n",
    "Normalize content\n",
    "        â†“\n",
    "formatted_history (clean)\n",
    "        â†“\n",
    "Send to model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#  Interview-Level Explanation (Short Version)\n",
    "\n",
    "Why do we loop through history?\n",
    "\n",
    "1. Gradio stores content as list (multimodal).\n",
    "2. LLM APIs expect plain string.\n",
    "3. So we extract text safely before sending to model.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#  Final Mental Model\n",
    "\n",
    "Think of this like:\n",
    "\n",
    "```\n",
    "history = complex UI format\n",
    "formatted_history = clean API format\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca7573",
   "metadata": {},
   "source": [
    "\n",
    "#  The Code\n",
    "\n",
    "```python\n",
    "messages = (\n",
    "    [{\"role\": \"system\", \"content\": system_message}]\n",
    "    + formatted_history\n",
    "    + [{\"role\": \"user\", \"content\": message_text}]\n",
    ")\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "\n",
    "return {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response[\"message\"][\"content\"]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#  Step 1: Creating `messages`\n",
    "\n",
    "## ðŸ”¹ What Is `messages`?\n",
    "\n",
    "It is a **list of dictionaries**.\n",
    "\n",
    "Modern LLM APIs expect input in this format:\n",
    "\n",
    "```python\n",
    "[\n",
    "  {\"role\": \"system\", \"content\": \"...\"},\n",
    "  {\"role\": \"user\", \"content\": \"...\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"...\"}\n",
    "]\n",
    "```\n",
    "\n",
    "This is the standard conversational structure used by:\n",
    "\n",
    "* Ollama\n",
    "* OpenAI API\n",
    "* Mistral API\n",
    "* DeepSeek API\n",
    "* Almost all chat-based LLMs\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ Breaking Down the List Concatenation\n",
    "\n",
    "```python\n",
    "[{\"role\": \"system\", \"content\": system_message}]\n",
    "+ formatted_history\n",
    "+ [{\"role\": \"user\", \"content\": message_text}]\n",
    "```\n",
    "\n",
    "This is **list concatenation** in Python.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "a = [1]\n",
    "b = [2,3]\n",
    "c = [4]\n",
    "\n",
    "result = a + b + c\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "[1,2,3,4]\n",
    "```\n",
    "\n",
    "Same concept here.\n",
    "\n",
    "---\n",
    "\n",
    "#  Step 2: What Each Part Means\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1ï¸âƒ£ System Message\n",
    "\n",
    "```python\n",
    "{\"role\": \"system\", \"content\": system_message}\n",
    "```\n",
    "\n",
    "This controls AI behavior.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "system_message = \"You are a helpful coding assistant.\"\n",
    "```\n",
    "\n",
    "This influences:\n",
    "\n",
    "* Tone\n",
    "* Personality\n",
    "* Response style\n",
    "* Rules\n",
    "\n",
    "ðŸ’¡ The model reads system message first.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2ï¸âƒ£ `formatted_history`\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "[\n",
    "  {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Hello!\"}\n",
    "]\n",
    "```\n",
    "\n",
    "This gives conversation memory.\n",
    "\n",
    "Without this â†’ model forgets previous messages.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3ï¸âƒ£ Current User Message\n",
    "\n",
    "```python\n",
    "{\"role\": \"user\", \"content\": message_text}\n",
    "```\n",
    "\n",
    "This is the new question.\n",
    "\n",
    "---\n",
    "\n",
    "# Final `messages` Example\n",
    "\n",
    "Suppose:\n",
    "\n",
    "```\n",
    "system_message = \"You are helpful.\"\n",
    "message_text = \"Explain AI.\"\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are helpful.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Hello!\"},\n",
    "  {\"role\": \"user\", \"content\": \"Explain AI.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "This is exactly what the LLM sees.\n",
    "\n",
    "---\n",
    "\n",
    "#  Step 3: Calling Ollama\n",
    "\n",
    "```python\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "```\n",
    "\n",
    "This sends:\n",
    "\n",
    "* Model name (e.g., `\"phi3\"`)\n",
    "* Entire conversation list\n",
    "\n",
    "To Ollama runtime.\n",
    "\n",
    "Ollama processes it and returns:\n",
    "\n",
    "```python\n",
    "{\n",
    "   \"model\": \"phi3\",\n",
    "   \"message\": {\n",
    "       \"role\": \"assistant\",\n",
    "       \"content\": \"AI stands for Artificial Intelligence...\"\n",
    "   }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#  Step 4: Extract Model Reply\n",
    "\n",
    "```python\n",
    "response[\"message\"][\"content\"]\n",
    "```\n",
    "\n",
    "Breakdown:\n",
    "\n",
    "```python\n",
    "response[\"message\"]\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "```python\n",
    "{\n",
    "   \"role\": \"assistant\",\n",
    "   \"content\": \"AI stands for Artificial Intelligence...\"\n",
    "}\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "[\"content\"]\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "```python\n",
    "\"AI stands for Artificial Intelligence...\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ Step 5: Return in Gradio Format\n",
    "\n",
    "```python\n",
    "return {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response[\"message\"][\"content\"]\n",
    "}\n",
    "```\n",
    "\n",
    "Why return dictionary?\n",
    "\n",
    "Because new **Gradio** expects:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"content\": \"text\"\n",
    "}\n",
    "```\n",
    "\n",
    "If you return just string:\n",
    "\n",
    "```\n",
    "ValidationError\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§  Architecture Flow\n",
    "\n",
    "```\n",
    "User types message\n",
    "        â†“\n",
    "Gradio sends message + history\n",
    "        â†“\n",
    "Build messages list\n",
    "        â†“\n",
    "Send to Ollama\n",
    "        â†“\n",
    "Get response\n",
    "        â†“\n",
    "Return in Gradio format\n",
    "        â†“\n",
    "UI displays assistant reply\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#  Why This Pattern Is Important\n",
    "\n",
    "This is called:\n",
    "\n",
    "> Role-Based Chat Formatting\n",
    "\n",
    "It allows the model to understand:\n",
    "\n",
    "* Who is speaking\n",
    "* What is instruction\n",
    "* What is past memory\n",
    "* What is current question\n",
    "\n",
    "Without roles, conversation quality drops.\n",
    "\n",
    "---\n",
    "\n",
    "#  Interview-Level Summary (3 Points)\n",
    "\n",
    "If interviewer asks:\n",
    "\n",
    "How does chat API work internally?\n",
    "\n",
    "You say:\n",
    "\n",
    "1. We construct a list of role-based messages (system, user, assistant).\n",
    "2. We append history + new input to maintain context.\n",
    "3. We send entire conversation to model, which generates next assistant response.\n",
    "\n",
    "---\n",
    "\n",
    "# Final Mental Model\n",
    "\n",
    "Think of `messages` as:\n",
    "\n",
    "```\n",
    "Full conversation transcript\n",
    "```\n",
    "\n",
    "And the model generates:\n",
    "\n",
    "```\n",
    "Next assistant reply\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ab706f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## OK let's keep going!\n",
    "\n",
    "Using a system message to add context, and to give an example answer.. this is \"one shot prompting\" again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "MODEL = \"gemma3:270m\"\n",
    "def chat(message, history):\n",
    "    print(message)\n",
    "    print(history)\n",
    "    print(type(history))\n",
    "    # #  Extract text safely (new Gradio format)\n",
    "    # if isinstance(message, dict):\n",
    "    #     message_text = message[\"content\"][0][\"text\"]\n",
    "    # elif isinstance(message, list):\n",
    "    #     message_text = message[0][\"text\"]\n",
    "    # else:\n",
    "    message_text = message\n",
    "\n",
    "    # Convert history to Ollama format\n",
    "    formatted_history = []\n",
    "    a = [{\"role\" : msg[\"role\"], \"content\" : msg[\"content\"][0][\"text\"]} for msg in history]\n",
    "    print(\"a \", a)\n",
    "\n",
    "    for msg in history:\n",
    "        if isinstance(msg[\"content\"], list):\n",
    "            text = msg[\"content\"][0][\"text\"]\n",
    "        else:\n",
    "            text = msg[\"content\"]\n",
    "\n",
    "        formatted_history.append({\n",
    "            \"role\": msg[\"role\"],\n",
    "            \"content\": text\n",
    "        })\n",
    "    print(\"hist \",formatted_history)\n",
    "    messages = (\n",
    "        [{\"role\": \"system\", \"content\": system_message}]\n",
    "        + formatted_history\n",
    "        + [{\"role\": \"user\", \"content\": message_text}]\n",
    "    )\n",
    "    print(\"msg \",messages)\n",
    "\n",
    "    response = ollama.chat(model=MODEL, messages=messages)\n",
    "    print(\"res \",response)\n",
    "    with open(\"response.json\", \"w\") as f:\n",
    "        json.dump(response.model_dump(), f, indent=4)\n",
    "    #  Return in Gradio expected format\n",
    "    return {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response[\"message\"][\"content\"]\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res  model='phi3' created_at='2026-02-17T17:58:06.5311164Z' done=True done_reason='stop' total_duration=90518133300 load_duration=5252818400 prompt_eval_count=160 prompt_eval_duration=15555028000 eval_count=291 eval_duration=69186407900 message=Message(role='assistant', content=\"Hello there! Welcome to our fashion-forward store where style meets savings. We've got a special promotion going on right now: Hats have been slashed by an amazing 60% off, making them the steal of the season here in Paris. If you're eyeing some trendy headwear or just looking to top off your ensemble with something chic and affordable, why not take a peek at our hat collection? They are crafted by renowned designers from around the globe â€“ styles ranging from classic fedoras for timeless looks to avant-garde beanies that make bold statements. Plus, we have exclusive Parisian designs you won't find anywhere else!\\n\\nIf hats arenâ€™t your thing or if there is something specific on your mind like shoes, I assure you the rest of our collection doesn't disappoint either â€“ all items are currently 50% off for that perfect balance between luxury and value. While we don't have a discounted range of footwear today, why not consider stepping into one of these incredible fashion finds to complete your look? Whether youâ€™re after boots with edgy designs or elegant high heels â€“ they all await at our store for that touch of Parisian chic.\\n\\nShould I assist in any other way, feel free to ask!\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\queueing.py\", line 766, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\route_utils.py\", line 355, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\blocks.py\", line 2152, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\blocks.py\", line 1627, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\utils.py\", line 1003, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 544, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 905, in _submit_fn\n",
      "    response = await run_sync(self.fn, *inputs, limiter=self.limiter)  # type: ignore\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\anyio\\to_thread.py\", line 63, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2502, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 986, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_7064\\4168392374.py\", line 41, in chat\n",
      "    json.dump(response)\n",
      "    ~~~~~~~~~^^^^^^^^^^\n",
      "TypeError: dump() missing 1 required positional argument: 'fp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "[]\n",
      "<class 'list'>\n",
      "a  []\n",
      "hist  []\n",
      "msg  [{'role': 'system', 'content': \"You are a helpful assistant in a clothes store. You should try to gently encourage the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. For example, if the customer says 'I'm looking to buy a hat', you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'Encourage the customer to buy hats if they are unsure what to get.\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, but remind the customer to look at hats!\"}, {'role': 'user', 'content': 'hello'}]\n",
      "res  model='gemma3:270m' created_at='2026-02-17T17:58:12.6095641Z' done=True done_reason='stop' total_duration=1821276200 load_duration=199442200 prompt_eval_count=142 prompt_eval_duration=60382100 eval_count=84 eval_duration=1475249100 message=Message(role='assistant', content=\"Hi there! Thank you for your interest in our sale deals. We have a fantastic selection of hats, including many that are part of our sales event. We've got hats that are 60% off, and most other items are 50% off.\\n\\nWould you like to try some of our hats today? We've got a variety of styles and colors to suit all tastes.\\n\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\queueing.py\", line 766, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\route_utils.py\", line 355, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\blocks.py\", line 2152, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\blocks.py\", line 1627, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\utils.py\", line 1003, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 544, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 905, in _submit_fn\n",
      "    response = await run_sync(self.fn, *inputs, limiter=self.limiter)  # type: ignore\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\anyio\\to_thread.py\", line 63, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2502, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\Documents\\genai\\day10\\myenv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 986, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_7064\\2403706661.py\", line 42, in chat\n",
      "    json.dump(response, f, indent=4)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\json\\__init__.py\", line 181, in dump\n",
      "    for chunk in iterable:\n",
      "                 ^^^^^^^^\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\json\\encoder.py\", line 451, in _iterencode\n",
      "    newobj = _default(o)\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\json\\encoder.py\", line 182, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "                    f'is not JSON serializable')\n",
      "TypeError: Object of type ChatResponse is not JSON serializable\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
