# ğŸ“˜ Study Notes: Parameters, Model Size & Scaling in LLMs

---

## ğŸ”¹ 1. Why â€œParametersâ€ Matter in LLMs

- You often hear statements like:
  - **LLaMA 3.2 â†’ 8 billion parameters**
  - **Gemma (smallest) â†’ 270 million parameters**

- Parameters are one of the **most important ideas** behind how powerful an LLM is

ğŸ“Œ Understanding parameters helps you understand:

- Model size
- Cost
- Capability
- Why models come in â€œnano / mini / largeâ€ versions

---

## ğŸ§  2. What Are Parameters?

### ğŸ”¸ Simple Definition

**Parameters** are the **learned numbers** inside a neural network that:

- Store information from training data
- Control how the model transforms input into output

ğŸ‘‰ They are like the **knowledge storage** of the model.

---

### ğŸ”¸ Intuition

- Parameters decide:
  - Which words relate to each other
  - Which patterns matter
  - How strongly one concept influences another

ğŸ“Œ More parameters â†’ more capacity to learn patterns.

---

## ğŸ§® 3. Parameters in Traditional ML vs Deep Learning

### ğŸ”¹ Traditional Machine Learning

- Typical models used:
  - **20â€“200 parameters**

- Example:
  - Credit scoring model
  - Uses 20â€“30 factors (income, age, history)

---

### ğŸ”¹ Deep Neural Networks

- Parameters jumped dramatically
- Early deep models:
  - Millions of parameters

- Modern LLMs:
  - **Billions to trillions**

ğŸ“Œ This jump shocked early practitioners.

---

## ğŸš€ 4. Parameter Explosion: GPT Timeline

| Model                      | Parameters                         |
| -------------------------- | ---------------------------------- |
| **GPT-1**                  | 117 million                        |
| **GPT-2**                  | 1.5 billion                        |
| **GPT-3**                  | 175 billion                        |
| **GPT-4**                  | ~1.76 trillion                     |
| **Latest frontier models** | Unknown (likely tens of trillions) |

ğŸ“Œ Labs no longer reveal exact counts.

---

## ğŸ“‰ 5. Doing More With Fewer Parameters

- Example:
  - **Gemma (270M)** > **GPT-2 (1.5B)** in capability

- Reason:
  - Better architectures
  - Better training methods
  - Better data efficiency

ğŸ“Œ **Efficiency has improved**, not just size.

---

## ğŸ§  6. General Rule (Important)

> **More parameters usually mean a more capable model**

Because:

- More parameters â†’ more training data absorbed
- More representational capacity

âš ï¸ But:

- Architecture and training quality also matter

---

## ğŸ§© 7. Why Models Come in Different Sizes

### Examples:

- **GPT-5**:
  - Nano
  - Mini
  - Full

- **Claude**:
  - Haiku (small)
  - Sonnet (medium)
  - Opus (large)

### Why this exists:

- Different parameter counts
- Different compute costs
- Different speed vs intelligence trade-offs

ğŸ“Œ Bigger models cost more to run.

---

## ğŸ’° 8. Parameters & Cost

- Cost comes from:
  - Computing trillions of parameters

- Larger model:
  - Higher API cost
  - Slower inference

- Smaller model:
  - Faster
  - Cheaper

ğŸ“Œ This is why pricing tiers exist.

---

## ğŸ“ˆ 9. Training-Time Scaling

### ğŸ”¸ What Is Training-Time Scaling?

- Increasing:
  - Model size
  - Number of parameters

- Requires:
  - More compute
  - More money
  - More training data

ğŸ“Œ Bigger model â†’ longer and more expensive training.

---

### ğŸ”¸ Chinchilla Scaling Laws (Background)

- Suggest:
  - Optimal relationship between:
    - Parameters
    - Training data size

- Rough idea:
  - Bigger models need proportionally more data

ğŸ“Œ Less discussed today, but foundational.

---

## âš™ï¸ 10. Inference-Time Scaling (Very Important)

### ğŸ”¸ What Is Inference?

- **Inference** = running a trained model
- Happens after training is complete

---

### ğŸ”¸ Inference-Time Scaling Techniques

#### 1ï¸âƒ£ Reasoning Techniques

- Ask model to:
  - â€œThink step by stepâ€

- Budget forcing:
  - Insert **â€œwaitâ€**
  - Force deeper reasoning

ğŸ“Œ Improves output quality without retraining.

---

#### 2ï¸âƒ£ More Context (Input Data)

- Provide:
  - Ticket prices
  - Policies
  - Documents

- Model draws from input sequence

ğŸ“Œ This leads directly to **RAG**.

---

## ğŸ†š 11. Training-Time vs Inference-Time Scaling

| Aspect      | Training-Time Scaling | Inference-Time Scaling   |
| ----------- | --------------------- | ------------------------ |
| When        | During training       | While using model        |
| Method      | More parameters       | More reasoning / context |
| Cost        | Very high             | Much lower               |
| Flexibility | Fixed                 | Adjustable per query     |

ğŸ“Œ These are **orthogonal approaches** â€” you can use both.

---

## ğŸ“Š 12. Logarithmic Scale of Model Sizes

### Important Note

- Parameter charts use **logarithmic scale**
- Each step = **10Ã— increase**

ğŸ“Œ 1B â†’ 10B â†’ 100B â†’ 1T

---

## ğŸ§ª 13. Open-Source Model Sizes (Examples)

- **LLaMA 3.2** â†’ 3B parameters
- **LLaMA 3.1** â†’ 8B (stronger)
- **LLaMA 3.3** â†’ 3.3B
- **LLaMA 4** â†’ multimodal (~2.45B variant)
- **GPT-OSS**:
  - 20B
  - 120B

- **DeepSeek**:
  - 671B parameters

ğŸ“Œ Many large models use **Mixture of Experts (MoE)**.

---

## ğŸ§  14. Mixture of Experts (MoE)

### ğŸ”¸ What Is MoE?

- Model contains:
  - Many smaller sub-models

- Only some activate per query

ğŸ“Œ Efficient way to scale without using all parameters every time.

---

## ğŸ¤¯ 15. Take a Step Back: Scale Is Enormous

- Modern frontier models:
  - Likely **tens of trillions** of parameters

- This scale is:
  - Hard to imagine
  - Unprecedented in computing history

ğŸ“Œ This is why LLM behavior can feel magical.

---

## âš¡ Quick Revision (Must Remember)

- **Parameters** = learned numbers storing knowledge
- Traditional ML: tens of parameters
- LLMs: billions â†’ trillions
- More parameters â†’ more capacity (usually)
- Models come in sizes due to **cost vs capability**
- **Training-time scaling** = bigger model
- **Inference-time scaling** = better prompting, reasoning, RAG
- Logarithmic scale = each step is 10Ã— bigger
- MoE models activate only parts of the network
