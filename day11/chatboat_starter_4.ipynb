{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import gradio as gr\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gemma3:270m\"\n",
    "# Again, I'll be in scientist-mode and change this global during the lab\n",
    "\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "## And now, writing a new callback\n",
    "\n",
    "We now need to write a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "Which will be a callback function we will give gradio.\n",
    "\n",
    "### The job of this function\n",
    "\n",
    "Take a message, take the prior conversation, and return the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ce793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    return \"bananas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f3417",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4996e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def chat(message, history):\n",
    "    with open(\"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "    return f\"You said {message} and the history is {history} but I still say bananas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434a0417",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890cac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7330d7f",
   "metadata": {},
   "source": [
    "## OK! Let's write a slightly better chat callback!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def chat(message, history):\n",
    "    print(message)\n",
    "\n",
    "    # Convert history to Ollama format\n",
    "    formatted_history = []\n",
    "\n",
    "    for msg in history:\n",
    "        if isinstance(msg[\"content\"], list):\n",
    "            text = msg[\"content\"][0][\"text\"]\n",
    "        else:\n",
    "            text = msg[\"content\"]\n",
    "\n",
    "        formatted_history.append({\n",
    "            \"role\": msg[\"role\"],\n",
    "            \"content\": text\n",
    "        })\n",
    "\n",
    "    messages = (\n",
    "        [{\"role\": \"system\", \"content\": system_message}]\n",
    "        + formatted_history\n",
    "        + [{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "\n",
    "    response = ollama.chat(model=MODEL, messages=messages)\n",
    "    \n",
    "    #  Return in Gradio expected format\n",
    "    return {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response[\"message\"][\"content\"]\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab706f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "# Chat Boat Assistant for Clothing Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"phi3\"\n",
    "system_message = \"\"\"Here‚Äôs a strong **system prompt** you can use for your store assistant:\n",
    "\n",
    "---\n",
    "\n",
    "### üìå System Prompt\n",
    "\n",
    "You are a witty, slightly snarky sales assistant working at a clothing store.\n",
    "\n",
    "The store currently only sells:\n",
    "\n",
    "* Clothes\n",
    "* Belts\n",
    "\n",
    "Both are available at **50% off**.\n",
    "\n",
    "Your personality:\n",
    "\n",
    "* Playfully sarcastic\n",
    "* Confident\n",
    "* Persuasive\n",
    "* Slightly dramatic\n",
    "* Never rude or offensive\n",
    "* Always funny in a charming way\n",
    "\n",
    "Your goals:\n",
    "\n",
    "1. Attract customers to buy clothes or belts.\n",
    "2. Constantly remind them about the 50% discount in creative ways.\n",
    "3. Upsell confidently.\n",
    "4. If the user asks for anything other than clothes or belts, respond with a snarky but playful reply saying it‚Äôs not available.\n",
    "5. Never break character.\n",
    "6. Never say you are an AI.\n",
    "7. Keep responses short, punchy, and engaging.\n",
    "\n",
    "Examples of tone:\n",
    "\n",
    "* If user asks for shoes ‚Üí\n",
    "  ‚ÄúShoes? Oh sweetheart, this is a *fashion establishment*, not a miracle factory. We do clothes and belts ‚Äî both 50% off. You‚Äôre welcome.‚Äù\n",
    "\n",
    "* If user says just ‚ÄúHi‚Äù ‚Üí\n",
    "  ‚ÄúWelcome to the land of 50% off brilliance. Clothes? Belts? Your wardrobe is begging.‚Äù\n",
    "\n",
    "* If user asks for a laptop ‚Üí\n",
    "  ‚ÄúA laptop? Bold of you. But unless it‚Äôs wearable or holds up your pants, we don‚Äôt stock it. Clothes and belts ‚Äî half price. Focus.‚Äù\n",
    "\n",
    "Always redirect the conversation back to:\n",
    "**Clothes. Belts. 50% off.**\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1365321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import ollama\n",
    "def chat(message, history):\n",
    "    \n",
    "    #  Extract text safely (new Gradio format)\n",
    "    if isinstance(message, dict):\n",
    "        message_text = message[\"content\"][0][\"text\"]\n",
    "    elif isinstance(message, list):\n",
    "        message_text = message[0][\"text\"]\n",
    "    else:\n",
    "        message_text = message\n",
    "    with open(\"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "    # Convert history to Ollama format\n",
    "    formatted_history = []\n",
    "\n",
    "    for msg in history:\n",
    "        if isinstance(msg[\"content\"], list):\n",
    "            text = msg[\"content\"][0][\"text\"]\n",
    "        else:\n",
    "            text = msg[\"content\"]\n",
    "\n",
    "        formatted_history.append({\n",
    "            \"role\": msg[\"role\"],\n",
    "            \"content\": text\n",
    "        })\n",
    "\n",
    "    messages = (\n",
    "        [{\"role\": \"system\", \"content\": system_message}]\n",
    "        + formatted_history\n",
    "        + [{\"role\": \"user\", \"content\": message_text}]\n",
    "    )\n",
    "\n",
    "    response = ollama.chat(model=MODEL, messages=messages)\n",
    "    \n",
    "    #  Return in Gradio expected format\n",
    "    return {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response[\"message\"][\"content\"]\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b898531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05eeeb",
   "metadata": {},
   "source": [
    "# Using Groq API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ce3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "client = Groq(api_key=\"your api key\")\n",
    "def chat(message, history):\n",
    "    \n",
    "    #  Extract text safely (new Gradio format)\n",
    "    if isinstance(message, dict):\n",
    "        message_text = message[\"content\"][0][\"text\"]\n",
    "    elif isinstance(message, list):\n",
    "        message_text = message[0][\"text\"]\n",
    "    else:\n",
    "        message_text = message\n",
    "\n",
    "    # Convert history to Ollama format\n",
    "    formatted_history = []\n",
    "\n",
    "    for msg in history:\n",
    "        if isinstance(msg[\"content\"], list):\n",
    "            text = msg[\"content\"][0][\"text\"]\n",
    "        else:\n",
    "            text = msg[\"content\"]\n",
    "\n",
    "        formatted_history.append({\n",
    "            \"role\": msg[\"role\"],\n",
    "            \"content\": text\n",
    "        })\n",
    "\n",
    "    messages = (\n",
    "        [{\"role\": \"system\", \"content\": system_message}]\n",
    "        + formatted_history\n",
    "        + [{\"role\": \"user\", \"content\": message_text}]\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    #  Return in Gradio expected format\n",
    "    return {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response.choices[0].message.content\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99824ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
